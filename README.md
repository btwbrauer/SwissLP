# ğŸ‡¨ğŸ‡­ Swiss Language Processing (SwissLP)

A minimal, best-practice repository for Text and Speech Processing using Swiss German and German language models.

## ğŸ“š Course: Essentials in Text and Speech Processing

This project provides a clean, reproducible development environment for Swiss German dialect classification.

## ğŸ“Š Dataset

This project uses the **SwissDial** dataset for Swiss German dialect classification tasks.

## ğŸš€ Features

### Text Models

- **SwissBERT**: BERT model fine-tuned on Swiss German data
- **German BERT**: Standard BERT model for German language
- **XLM-RoBERTa**: Cross-lingual model supporting 100+ languages
- **fastText**: Pre-trained word embeddings (located in `crawl-300d-2M-subword/`)

### Speech Models

- **Wav2Vec2**: Facebook's self-supervised speech representation
- **AST**: Audio Spectrogram Transformer
- **Whisper**: OpenAI's robust speech recognition model

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.10 or higher
- pip

### Setup

1. **Clone the repository:**

```bash
git clone https://github.com/btwbrauer/SwissLP.git
cd SwissLP
```

1. **Create virtual environment:**

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

1. **Install dependencies:**

```bash
pip install -e .
```

1. **Verify installation:**

```bash
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
pytest tests/ -v
```

**Note**: The fastText embeddings are located in the `crawl-300d-2M-subword/` directory. This directory is gitignored due to file size.

## ğŸ“ Project Structure

```text
SwissLP/
â”œâ”€â”€ pyproject.toml         # Python dependencies and project configuration
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/            # Model loaders
â”‚   â”‚   â”œâ”€â”€ text_models.py    # Text model loaders
â”‚   â”‚   â””â”€â”€ speech_models.py # Speech model loaders
â”‚   â”œâ”€â”€ evaluation/        # Evaluation utilities
â”‚   â”‚   â”œâ”€â”€ evaluator.py      # Evaluator classes
â”‚   â”‚   â””â”€â”€ metrics.py        # Metrics computation
â”‚   â”œâ”€â”€ config/            # Configuration management
â”‚   â”‚   â””â”€â”€ config.py         # Config classes and YAML loading
â”‚   â”œâ”€â”€ training/          # Training utilities
â”‚   â”‚   â”œâ”€â”€ trainer.py        # Trainer classes
â”‚   â”‚   â”œâ”€â”€ hyperparameter_tuning.py # Optuna hyperparameter tuning
â”‚   â”‚   â””â”€â”€ optimization.py  # Model optimization workflow
â”‚   â””â”€â”€ utils/             # Utility functions
â”‚       â”œâ”€â”€ dataset.py        # Dataset loading and preparation
â”‚       â””â”€â”€ mlflow_utils.py   # MLflow integration
â”œâ”€â”€ scripts/               # Executable scripts
â”‚   â””â”€â”€ optimize_models.py   # Hyperparameter optimization script
â”œâ”€â”€ configs/               # Configuration files
â”‚   â”œâ”€â”€ swissbert.yaml
â”‚   â”œâ”€â”€ german_bert.yaml
â”‚   â”œâ”€â”€ xlm_roberta.yaml
â”‚   â””â”€â”€ wav2vec2.yaml
â””â”€â”€ tests/                 # Test suite
```

## ğŸ“ Training Models

Fine-tune models on the Swiss German dialect dataset using hyperparameter optimization:

```bash
python scripts/optimize_models.py --models swissbert --n-trials 20
```

```bash
python scripts/optimize_models.py --models german_bert --n-trials 20
```

```bash
python scripts/optimize_models.py --models xlm_roberta --n-trials 20
```

```bash
python scripts/optimize_models.py --models swissbert german_bert xlm_roberta --n-trials 20
```

```bash
python scripts/optimize_models.py --models swissbert --n-trials 30 --timeout 7200
```

```bash
python scripts/optimize_models.py --models swissbert --n-trials 20 --skip-cleanup
```

The optimization script will:

- Run hyperparameter tuning using Optuna
- Automatically log all trials to MLflow
- Keep only the best model (unless `--skip-cleanup` is used)
- Save optimized config files to `configs/`

## ğŸ”¬ Evaluating and Comparing Models

After training, evaluate your fine-tuned models. Evaluation is done programmatically using the evaluation utilities in `src/evaluation/`. Load trained models from the `outputs/` directory and evaluate them on the test dataset.

The evaluation module provides:

- `TextEvaluator` and `SpeechEvaluator` classes
- Comprehensive metrics (accuracy, precision, recall, F1-score)
- Per-class metrics computation
- Results saving to JSON/CSV

## ğŸ“Š MLflow Experiment Tracking

All training runs are automatically logged to MLflow for experiment tracking and comparison.

### MLflow Configuration

By default, training scripts log to <http://localhost:5000> (remote MLflow server).

**To use a different MLflow server**, set the `MLFLOW_TRACKING_URI` environment variable:

```bash
export MLFLOW_TRACKING_URI="http://localhost:5000"
```

```bash
export MLFLOW_TRACKING_URI="./mlruns"
```

### What Gets Logged

MLflow automatically tracks:

- **Parameters**: Model configuration, hyperparameters, data splits
- **Metrics**: Training loss, validation metrics (accuracy, F1, etc.)
- **Artifacts**: Trained models, checkpoints, config files
- **Experiment metadata**: Run name, timestamp, experiment name

## âš™ï¸ Configuration

Model training is configured using YAML files in the `configs/` directory.

Key configuration sections:

- **model**: Model name, number of labels, device
- **data**: Data path, dialects, train/val/test splits, batch_size
- **training**: Learning rate, batch size, epochs, early stopping, etc.
- **experiment_name**: Name for organizing outputs

## ğŸ§ª Testing

Run all tests:

```bash
pytest tests/ -v
```

Run with coverage:

```bash
pytest tests/ --cov=src --cov-report=html
```

## ğŸ”¬ Development

### Code Quality

Format code:

```bash
ruff format src/ tests/
```

Lint code:

```bash
ruff check src/ tests/
```

## ğŸ“¦ Dependencies

See `pyproject.toml` for the complete list of dependencies with exact versions.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `pytest tests/ -v`
5. Format code: `ruff format src/ tests/`
6. Submit a pull request

## ğŸ™ Acknowledgments

- **Hugging Face**: For the transformers library and model hub
- **PyTorch Team**: For the deep learning framework
- **Course Instructors**: For guidance and support

## ğŸ“š Resources

- [Hugging Face Models](https://huggingface.co/models)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [SwissBERT Paper](https://aclanthology.org/2023.swisstext-1.3/)
